<html>
<head>
<meta charset="UTF-8">
<link rel="stylesheet" href="file:/C:/Users/QuXian/.markdownNavigator/multimarkdown_layout.css">
<style>

body.multimarkdown-preview,
body.multimarkdown-wiki-preview {
    font-size: 14px;
}
</style>
<link rel="stylesheet" href="file:/C:/Users/QuXian/.markdownNavigator/multimarkdown_darcula.css">
</head>
<body class="multimarkdown-preview">
<div class="content">
<div class="page-header"><a href="https://github.com/SherlockHolmes221/CNNs/blob/master/note.md" name="wikipage" id="wikipage" title="Click here to open the file on GitHub">note.md</a></div>
<div class="hr"></div>
<h1 id="return-of-the-devil-in-the-details-delving-deep-into-convolutional-nets2014" md-pos="2-80"><a href="#return-of-the-devil-in-the-details-delving-deep-into-convolutional-nets2014" name="return-of-the-devil-in-the-details-delving-deep-into-convolutional-nets2014">Return of the Devil in the Details: Delving Deep into Convolutional Nets(2014)</a></h1>
<h4 id="主要是cnn和ifv为代表的浅层特征表示方法对图像分类效果的对比" md-pos="86-118"><a href="#主要是cnn和ifv为代表的浅层特征表示方法对图像分类效果的对比" name="主要是cnn和ifv为代表的浅层特征表示方法对图像分类效果的对比">主要是CNN和IFV为代表的浅层特征表示方法对图像分类效果的对比</a></h4>
<h4 id="以及图像处理细节colour-information-feature-normalisation-and-data-augmentation带来的影响" md-pos="124-203"><a href="#以及图像处理细节colour-information-feature-normalisation-and-data-augmentation带来的影响" name="以及图像处理细节colour-information-feature-normalisation-and-data-augmentation带来的影响">以及图像处理细节(colour information, feature normalisation, and data augmentation)带来的影响</a></h4>
<h5 id="主要结论" md-pos="211-216"><a href="#主要结论" name="主要结论">主要结论:</a></h5>
<ul>
  <li md-pos="217-284">CNN-based methods consistently outperform the shallow encodings.</li>
  <li md-pos="284-385">The CNN output layer can be reduced significantly without having an adverse effect on performance.</li>
  <li md-pos="385-524">The performance of shallow representations can be significantly improved by adopting data augmentation, typically used in deep learning.</li>
  <li md-pos="524-623">L2-normalising the CNN features before use in the SVM was found to be important for performance.</li>
  <li md-pos="623-748">The performance of deep representations on the ILSVRC dataset is a good indicator of their performance on other datasets,</li>
  <li md-pos="748-886">Fine-tuning can further improve on already very strong results achieved using the combination of deep representations and a linear SVM.</li>
</ul>
<h5 id="实验细节" md-pos="893-898"><a href="#实验细节" name="实验细节">实验细节:</a></h5>
<ul>
  <li md-pos="899-909">实验的网络结构</li>
</ul>
<p md-pos="910-987"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/devil_arch.png" alt="" md-pos="910-986" /></p>
<p md-pos="988-1008">(fc with dropout!!)</p>
<ul>
  <li md-pos="1008-1079" class="p">
    <p md-pos="1010-1079" class="p">数据集 voc2007(mAP) voc2012 ILSVRC-2012(top-5) Caltech-101  Caltech-256</p>
  </li>
  <li md-pos="1079-1360" class="p">
    <p md-pos="1081-1090" class="p">Training</p>
    <p md-pos="1094-1115">dataset: ILSVRC-2012</p>
    <p md-pos="1120-1153">momentum 0.9,weight decay 5·10−4</p>
    <p md-pos="1158-1255">initial learning rate 10−2 decreased by a factor of 10 when the validation error stop decreasing</p>
    <p md-pos="1260-1360">The layers are initialised from a Gaussian distribution with a zero mean and variance equal to 10−2</p>
  </li>
  <li md-pos="1360-1452" class="p">
    <p md-pos="1362-1387" class="p">The loss function of voc</p>
    <p md-pos="1390-1428">one-vs-rest classiﬁcation hinge loss</p>
    <p md-pos="1433-1452">ranking hinge loss</p>
  </li>
</ul>
<h5 id="实验结果和对比结论" md-pos="1461-1471"><a href="#实验结果和对比结论" name="实验结果和对比结论">实验结果和对比结论:</a></h5>
<p md-pos="1473-1552"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/devil_result.png" alt="" md-pos="1473-1551" /></p>
<ul>
  <li md-pos="1553-1862" class="p">
    <p md-pos="1555-1595" class="p">Ablation experiments1:data augmentation</p>
    <p md-pos="1598-1670">no augmentation,random crops, horizontal ﬂips, and RGB colour jittering</p>
    <p md-pos="1673-1750">Augmentation consistently improves performance by ∼ 3% for both IFV and CNN</p>
    <p md-pos="1755-1789">Fipping improves only marginally.</p>
    <p md-pos="1794-1862">The more expensive C+F sampling improves, as seen, by about 2 ∼ 3%.</p>
  </li>
  <li md-pos="1862-2031" class="p">
    <p md-pos="1864-1925" class="p">Ablation experiments2: CNN ﬁne-tuning vs without ﬁne-tuning</p>
    <p md-pos="1930-2031">Fine-tuning is able to adjust the learnt deep representation to better suit the dataset in question.</p>
  </li>
  <li md-pos="2031-2250" class="p">
    <p md-pos="2033-2087" class="p">Ablation experiments3:different loss functions of voc</p>
    <p md-pos="2092-2250">On VOC-2012, using the ranking loss is marginally better than the classification loss , which can be explained by the ranking-based VOC evaluation criterion.</p>
  </li>
  <li md-pos="2250-2440" class="p">
    <p md-pos="2252-2308" class="p">Ablation experiments4:different dimensions of fc layers</p>
    <p md-pos="2312-2440">We can reduce the output dimensionality further to 1024D and even 128D with only a drop of ∼ 2% for codes that are 32× smaller.</p>
  </li>
  <li md-pos="2440-2826" class="p">
    <p md-pos="2442-2483" class="p">Ablation experiments5:Colour information</p>
    <p md-pos="2486-2650">SIFT and colour descriptors are combined by stacking the corresponding IFVs,there is a small but significant improvement of around ∼ 1% in the non-augmented case.</p>
    <p md-pos="2656-2826">Retraining the network after converting all the input images to grayscale (denoted GS in Methods) has a more significant impact, resulting in a performance drop of ∼ 3%.</p>
  </li>
  <li md-pos="2826-3087" class="p">
    <p md-pos="2828-2899" class="p">Ablation experiments6:different architectures(3 kinds of CNNs and IFV)</p>
    <p md-pos="2902-2993">Both medium CNN-M and slow CNN-S outperform the fast CNN-F by a significant 2 ∼ 3% margin.</p>
    <p md-pos="2998-3087">An advantage of CNNs compared to IFV is the small dimensionality of the output features.</p>
  </li>
</ul>
<h5 id="something-questions" md-pos="3096-3116"><a href="#something-questions" name="something-questions">something questions:</a></h5>
<ul>
  <li md-pos="3117-3182">How data augmentation samples used sum/max-pooling or stacking</li>
  <li md-pos="3182-3209">How they choose CNN arch</li>
  <li md-pos="3209-3247">The details of loss function of voc</li>
  <li md-pos="3247-3276">IFV is hard to understand.</li>
</ul>
<h1 id="network-in-network2014-based-on-maxout-network" md-pos="3279-3327"><a href="#network-in-network2014-based-on-maxout-network" name="network-in-network2014-based-on-maxout-network">Network In Network(2014) based on maxout network</a></h1>
<p md-pos="3329-3399"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/nin.png" alt="" md-pos="3329-3398" /></p>
<p md-pos="3400-3487">NINs include the stacking of three mlpconv layers and one global average pooling layer</p>
<h4 id="nin" md-pos="3493-3497"><a href="#nin" name="nin">NIN:</a></h4>
<p md-pos="3499-3702">This structure consists of mlpconv layers which use multilayer perceptrons to convolve the input
and a global average pooling layer as a replacement for the fully connected layers in conventional CNN.</p>
<h4 id="the-shortness-of-convolutional-filter-and-fully-connected-layers" md-pos="3707-3771"><a href="#the-shortness-of-convolutional-filter-and-fully-connected-layers" name="the-shortness-of-convolutional-filter-and-fully-connected-layers">The shortness of convolutional filter and fully connected layers</a></h4>
<ul>
  <li md-pos="3772-3829">abstraction is low with generalized linear model (GLM)</li>
  <li md-pos="3829-3918">The data for the same concept often live on a nonlinear manifold.(how to comprehend??)</li>
  <li md-pos="3918-4018">The fully connected layers are prone to overfitting and heavily depend on dropout regularization.</li>
</ul>
<h4 id="what-is-mlp-convolution-layers" md-pos="4024-4054"><a href="#what-is-mlp-convolution-layers" name="what-is-mlp-convolution-layers">What is MLP Convolution Layers</a></h4>
<p md-pos="4056-4134"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/nin_formula.png" alt="" md-pos="4056-4133" /></p>
<p md-pos="4135-4234">1x1 convolution kernel and cascaded cross channel parametric pooling on a normal convolution layer</p>
<ul>
  <li md-pos="4235-4428">The mlpconv maps the input local patch to the output feature vector with a multi
    layer perceptron (MLP) consisting of multiple fully connected layers with nonlinear activation functions.</li>
</ul>
<h4 id="why-micro-mlp-convolution-layers-works" md-pos="4434-4473"><a href="#why-micro-mlp-convolution-layers-works" name="why-micro-mlp-convolution-layers-works">Why micro MLP Convolution Layers works?</a></h4>
<ul>
  <li md-pos="4474-4525">It is a general nonlinear function approximator.</li>
  <li md-pos="4525-4659">Multilayer perceptron can be a deep model,which is consistent with the spirit of feature re-use,不用像CNN那样靠卷积层的堆叠得到high level feature</li>
</ul>
<h4 id="why-global-average-pooling-works" md-pos="4665-4698"><a href="#why-global-average-pooling-works" name="why-global-average-pooling-works">Why global average pooling works?</a></h4>
<ul>
  <li md-pos="4699-5063">global average pooling is more meaningful and interpretable as it enforces correspondance between feature maps and categories
    instead of black box as fully connection layers.
    ( global average pooling over the fully connected layers is that it is more native to the convolution structure by
    enforcing correspondences between feature maps and categories. )</li>
  <li md-pos="5063-5298">Global average pooling is itself a structural regularizer, which natively prevents overfitting for the overall structure.
    (There is no parameter to optimize in the global average pooling thus overfitting is avoided at this layer. )</li>
  <li md-pos="5298-5419">Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.</li>
</ul>
<h4 id="something-questions-1" md-pos="5425-5445"><a href="#something-questions-1" name="something-questions-1">Something questions:</a></h4>
<ul>
  <li md-pos="5446-5500">Weather it works well when the mlpconv goes deeper?</li>
  <li md-pos="5500-5737">How to comprehend:
    Mlpconv layer differs from maxout layer in that the convex function approximator is replaced by a universal function approximator, which has greater capability in modeling various distributions of latent concepts.</li>
  <li md-pos="5737-5804">The details of data augmentation is not mentioned in this paper.</li>
</ul>
<h1 id="deeply-supervised-nets2015" md-pos="5807-5835"><a href="#deeply-supervised-nets2015" name="deeply-supervised-nets2015">Deeply-Supervised Nets(2015)</a></h1>
<p md-pos="5837-5907"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/dsn.png" alt="" md-pos="5837-5906" /></p>
<p md-pos="5908-5923">loss function:</p>
<p md-pos="5924-5999"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/dsn_loss.jpg" alt="" md-pos="5924-5998" /></p>
<p md-pos="6000-6052">companion objective functions for each hidden layer</p>
<h4 id="improve-the-problems-below" md-pos="6059-6086"><a href="#improve-the-problems-below" name="improve-the-problems-below">Improve the problems below:</a></h4>
<ul>
  <li md-pos="6087-6168">transparency in the effect intermediate layers have on overall classification.</li>
  <li md-pos="6168-6252">discriminativeness and robustness of learned features, especially in early layers</li>
  <li md-pos="6252-6314">training effectiveness in the face of “vanishing” gradients</li>
</ul>
<h4 id="why-it-works" md-pos="6320-6333"><a href="#why-it-works" name="why-it-works">Why it works:</a></h4>
<ul>
  <li md-pos="6334-6501">for small training data and relatively shallower networks, deep supervision
    functions as a strong “regularization” for classification accuracy and learned features</li>
  <li md-pos="6501-6726">for large training data and deeper networks deep supervision
    makes it convenient to exploit the significant performance gains that
    extremely deep networks can bring by improving otherwise problematic convergence behavior</li>
  <li md-pos="6726-7024">If the features in question are the hidden layer feature maps of a deep network,
    this observation means that the performance of a discriminative classifier trained
    using these hidden layer feature maps can serve as a proxy for the quality/discriminativeness
    of those hidden layer feature maps</li>
</ul>
<h4 id="experiment-details" md-pos="7030-7049"><a href="#experiment-details" name="experiment-details">Experiment details:</a></h4>
<ul>
  <li md-pos="7050-7117">SGD solver with mini-batch size of 128 and fixed momentum of 0.9</li>
  <li md-pos="7117-7161">Two dropout layers with dropout rate 0.5.</li>
</ul>
<h4 id="experiment-results" md-pos="7167-7186"><a href="#experiment-results" name="experiment-results">Experiment results:</a></h4>
<ul>
  <li md-pos="7187-7311">L2SVM and softmax and show DSN-L2SVM and DSN-Softmax yield gains over corresponding CNN-L2SVM and CNN-Softmax approaches</li>
  <li md-pos="7311-7422">Our DSN model achieves an error rate of 9.69% without data augmentation and of 7.97% with data augmentation</li>
</ul>
<h1 id="delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification2015" md-pos="7425-7522"><a href="#delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification2015" name="delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification2015">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification(2015)</a></h1>
<p md-pos="7524-7624">Propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit.</p>
<p md-pos="7625-7697"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/prelu.png" alt="" md-pos="7625-7696" /></p>
<p md-pos="7698-7794">Derive a robust initialization method that particularly considers the rectifier nonlinearities.</p>
<div md-pos="7795-8002">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/relu_init.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/prelu_init.png">
</figure>
</div>
<p md-pos="8004-8022">new architecture:</p>
<p md-pos="8023-8105"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/rectifiers_arch.png" alt="" md-pos="8023-8104" /></p>
<h4 id="some-problems" md-pos="8111-8124"><a href="#some-problems" name="some-problems">Some Problems</a></h4>
<ul>
  <li md-pos="8125-8167">What is channel-shared and channel-wise</li>
</ul>
<h1 id="highway-networks2015" md-pos="8170-8192"><a href="#highway-networks2015" name="highway-networks2015">Highway Networks(2015)</a></h1>
<p md-pos="8193-8445">by the use of gating units which learn to regulate the ﬂow of information through a network
Optimization of highway networks is virtually independent of depth,
while for traditional networks it suffers significantly as the number of layers increases.</p>
<h4 id="what-is-highway-networks" md-pos="8451-8475"><a href="#what-is-highway-networks" name="what-is-highway-networks">What is Highway Networks</a></h4>
<p md-pos="8477-8559"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/highway_formula.png" alt="" md-pos="8477-8558" /></p>
<p md-pos="8560-8643"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/highway_formula1.png" alt="" md-pos="8560-8642" /></p>
<h4 id="experiment-architecture" md-pos="8649-8672"><a href="#experiment-architecture" name="experiment-architecture">Experiment Architecture</a></h4>
<ul>
  <li md-pos="8673-8836">The first layer is always a regular fully-connected layer followed by 9, 19, 49, or 99 fully-connected plain or highway layers and a single softmax output layer</li>
  <li md-pos="8836-8943">The number of units in each layer is kept constant and it is 50 for highways and 71 for plain networks.</li>
</ul>
<h4 id="experiment-results-1" md-pos="8949-8967"><a href="#experiment-results-1" name="experiment-results-1">Experiment Results</a></h4>
<ul>
  <li md-pos="8968-9061">Highway networks on the other hand do not seem to suffer from an increase in depth at all.</li>
  <li md-pos="9061-9142">The highway networks always converge significantly faster than the plain ones.</li>
</ul>
<h4 id="some-problems-1" md-pos="9148-9162"><a href="#some-problems-1" name="some-problems-1">Some problems:</a></h4>
<ul>
  <li md-pos="9163-9371">What is the meaning of
    'Various activation functions which may be more suitable for particular problems
    but for which robust initialization schemes are unavailable can be used in deep highway networks. '</li>
</ul>
<h1 id="very-deep-convolutional-networks-for-large-scale-image-recognition2015" md-pos="9374-9446"><a href="#very-deep-convolutional-networks-for-large-scale-image-recognition2015" name="very-deep-convolutional-networks-for-large-scale-image-recognition2015">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION(2015)</a></h1>
<p md-pos="9447-9630">为了增加网络的深度 use very small (3×3) convolution filters, small receptive field
A stack of two 3×3 conv layers (without spatial pooling in between) has an effective receptive field of 5×5.</p>
<h4 id="why-cnns-enjoy-great-success" md-pos="9636-9665"><a href="#why-cnns-enjoy-great-success" name="why-cnns-enjoy-great-success">Why CNNs enjoy great success?</a></h4>
<ul>
  <li md-pos="9666-9704">The large public image repositories</li>
  <li md-pos="9704-9741">High-performance computing systems</li>
</ul>
<h4 id="architecture-details" md-pos="9747-9767"><a href="#architecture-details" name="architecture-details">Architecture details</a></h4>
<p md-pos="9769-9839"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/vgg.png" alt="" md-pos="9769-9838" /></p>
<ul>
  <li md-pos="9840-9896">Use filters with a very small receptive field: 3 × 3</li>
  <li md-pos="9896-10030">In one of the configurations utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels.</li>
  <li md-pos="10030-10121">Spatial pooling is carried out by ﬁve max-pooling layers, which follow some of the conv.</li>
  <li md-pos="10121-10261">A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers</li>
  <li md-pos="10261-10324">All hidden layers are equipped with the rectification (ReLU)</li>
  <li md-pos="10324-10418">Local Response Normalisation does not improve the performance on the ILSVRC dataset.(why??)</li>
</ul>
<h4 id="why-it-works-1" md-pos="10424-10437"><a href="#why-it-works-1" name="why-it-works-1">why it works:</a></h4>
<ul>
  <li md-pos="10438-10488">makes the decision function more discriminative</li>
  <li md-pos="10488-10524">decrease the number of parameters</li>
  <li md-pos="10524-10686">The incorporation of 1 × 1 conv layers is a way to increase the nonlinearity of the decision function without affecting the receptive fields of the conv layers</li>
</ul>
<h4 id="experiment-details-实验细节中对数据的预处理做得比较好" md-pos="10692-10728"><a href="#experiment-details-实验细节中对数据的预处理做得比较好" name="experiment-details-实验细节中对数据的预处理做得比较好">Experiment details 实验细节中对数据的预处理做得比较好</a></h4>
<ul>
  <li md-pos="10729-10940" class="p">
    <p md-pos="10731-10751" class="p">Data augmentation:</p>
    <p md-pos="10756-10849">pixel subtracting the mean RGB value, random horizontal flipping and random RGB colour shift</p>
    <p md-pos="10854-10940">randomly cropped from rescaled training images (one crop per image per SGD iteration)</p>
  </li>
  <li md-pos="10940-10957" class="p">
    <p md-pos="10942-10957" class="p">mini-batch 256</p>
  </li>
  <li md-pos="10957-11003" class="p">
    <p md-pos="10959-11003" class="p">SGD(momentum 0.9  weight decay(L2 5*10^-4))</p>
  </li>
  <li md-pos="11003-11134" class="p">
    <p md-pos="11005-11134" class="p">learning rate was initially set to 10−2, and then decreased by a factor of 10 when the validation set accuracy stopped improving</p>
  </li>
  <li md-pos="11134-11148" class="p">
    <p md-pos="11136-11148" class="p">dropout 0.5</p>
  </li>
</ul>
<h4 id="experiment" md-pos="11154-11164"><a href="#experiment" name="experiment">Experiment</a></h4>
<ul>
  <li md-pos="11166-11265">Local response normalisation does not improve on the model A without any normalisation layers.</li>
  <li md-pos="11265-11332">Classification error decreases with the increased ConvNet depth.</li>
  <li md-pos="11332-11411">A deep net with small filters outperforms a shallow net with larger filters.</li>
</ul>
<h1 id="going-deeper-with-convolutions2015-inception_v1" md-pos="11414-11463"><a href="#going-deeper-with-convolutions2015-inception_v1" name="going-deeper-with-convolutions2015-inception_v1">Going Deeper with Convolutions(2015) Inception_v1</a></h1>
<p md-pos="11465-11543"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv1.png" alt="" md-pos="11465-11542" /></p>
<p md-pos="11544-11622">Introduce sparsity and replace the fully connected layers by the sparse ones</p>
<p md-pos="11623-11717">Increased the depth and width of the network while keeping the computational budget constant.</p>
<h4 id="architecture-details-1" md-pos="11723-11743"><a href="#architecture-details-1" name="architecture-details-1">Architecture details</a></h4>
<p md-pos="11745-11828"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv1_arch.png" alt="" md-pos="11745-11827" /></p>
<ul>
  <li md-pos="11829-11973">In order to avoid patch-alignment issues,
    current incarnations of the Inception architecture are restricted to filter sizes 1×1, 3×3 and 5×5</li>
  <li md-pos="11973-12149">The suggested architecture is a combination of all those layers with their output filter
    banks concatenated into a single output vector forming the input of the next stage.</li>
  <li md-pos="12149-12234">The ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.</li>
  <li md-pos="12234-12342">Judiciously reducing dimension wherever the computational requirements would increase too much otherwise.</li>
  <li md-pos="12342-12445">All the convolutions, including those inside the Inception modules, use rectified linear activation.</li>
  <li md-pos="12445-12485">Average pooling before the classifier</li>
</ul>
<h4 id="why-it-works-2" md-pos="12491-12504"><a href="#why-it-works-2" name="why-it-works-2">Why it works:</a></h4>
<ul>
  <li md-pos="12505-12524">sparse structure</li>
  <li md-pos="12524-12584">combination of all those layers with their output filter</li>
  <li md-pos="12584-12631">adding an alternative parallel pooling path</li>
  <li md-pos="12631-12654">1 * 1:降维  构成了更有效的卷积层</li>
</ul>
<h4 id="experiment-details-1" md-pos="12660-12678"><a href="#experiment-details-1" name="experiment-details-1">Experiment details</a></h4>
<ul>
  <li md-pos="12679-12699">SGD(momentum=0.9)</li>
  <li md-pos="12699-12781">Fixed learning rate schedule(decreasing the learning rate by 4% every 8 epochs)</li>
  <li md-pos="12781-12896">Trained 7 versions of the same GoogLeNet model (including one wider version), and performed ensemble prediction.</li>
  <li md-pos="12896-12916">Testing:144-crops</li>
</ul>
<h4 id="experiment-result" md-pos="12922-12939"><a href="#experiment-result" name="experiment-result">Experiment result</a></h4>
<p md-pos="12941-13026"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv1_result.png" alt="" md-pos="12941-13025" /></p>
<h1 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift2015-inception_v2" md-pos="13029-13140"><a href="#batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift2015-inception_v2" name="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift2015-inception_v2">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift(2015) Inception_v2</a></h1>
<h4 id="what-is-batch-normalization" md-pos="13147-13175"><a href="#what-is-batch-normalization" name="what-is-batch-normalization">What is Batch Normalization:</a></h4>
<p md-pos="13177-13246"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/bn.png" alt="" md-pos="13177-13245" /></p>
<h4 id="what-problems-batch-normalization-solves" md-pos="13252-13293"><a href="#what-problems-batch-normalization-solves" name="what-problems-batch-normalization-solves">What problems Batch Normalization solves:</a></h4>
<ul>
  <li md-pos="13294-13618">internal covariate
    distribution of each layer’s inputs changes during training, as the parameters of the previous layers change.
    This slows down the training by requiring lower learning rates and careful parameter initialization,
    and makes it notoriously hard to train models with saturating nonlinearities.</li>
  <li md-pos="13618-13619"></li>
</ul>
<h4 id="why-batch-normalization-works" md-pos="13626-13656"><a href="#why-batch-normalization-works" name="why-batch-normalization-works">Why Batch Normalization works:</a></h4>
<ul>
  <li md-pos="13657-13731">Use much higher learningrates and be less careful about initialization.</li>
  <li md-pos="13731-13811">Acts as a regularizer, regularizes the model and reduces the need for Dropout</li>
  <li md-pos="13811-13932">Makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes</li>
  <li md-pos="13932-13984">Batch Normalization enables higher learning rates</li>
  <li md-pos="13984-14028">Batch Normalization regularizes the model</li>
</ul>
<h4 id="experiments" md-pos="14034-14046"><a href="#experiments" name="experiments">Experiments:</a></h4>
<ul>
  <li md-pos="14047-14183">By only using Batch Normalization ( BN-Baseline ), we match the accuracy of Inception in less than half the number of training steps.</li>
  <li md-pos="14183-14474">By applying the modifications(Increase learning rate, Remove Dropout, Reduce the photometric distortions,
    Reduce the L2 weight regularization, Accelerate the learning rate decay, Shuffle training examples more thoroughly),
    we significantly increase the training speed of the network.</li>
</ul>
<h1 id="rethinking-the-inception-architecture-for-computervision2015-a-little-hard-to-understand" md-pos="14477-14569"><a href="#rethinking-the-inception-architecture-for-computervision2015-a-little-hard-to-understand" name="rethinking-the-inception-architecture-for-computervision2015-a-little-hard-to-understand">Rethinking the Inception Architecture for ComputerVision(2015) (a little hard to understand)</a></h1>
<p md-pos="14570-14635">improve computational efficiency and reduce low parameter counts</p>
<h4 id="architecture" md-pos="14641-14653"><a href="#architecture" name="architecture">Architecture</a></h4>
<p md-pos="14655-14733"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3.png" alt="" md-pos="14655-14732" /></p>
<h4 id="some-design-principles" md-pos="14739-14761"><a href="#some-design-principles" name="some-design-principles">Some Design Principles</a></h4>
<ul>
  <li md-pos="14762-14833">Avoid representational bottlenecks, especially early in the network.</li>
  <li md-pos="14833-14917">Higher dimensional representations are easier to process locally within a network</li>
  <li md-pos="14917-15040">Spatial aggregation can be done over lower dimensional embeddings
    without much or any loss in representational power.</li>
  <li md-pos="15040-15085">Balance the width and depth of the network</li>
</ul>
<h4 id="improve-points" md-pos="15091-15105"><a href="#improve-points" name="improve-points">Improve points</a></h4>
<ul>
  <li md-pos="15106-15193">Factorization into smaller convolutions 5×5conv = 2×(3×3conv) 7×7conv = 3×(3×3conv)</li>
</ul>
<p md-pos="15194-15274"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_1.png" alt="" md-pos="15194-15273" /></p>
<div md-pos="15275-15489">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_2.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_3.png">
</figure>
</div>
<ul>
  <li md-pos="15491-15545">Spatial Factorization into Asymmetric Convolutions</li>
</ul>
<div md-pos="15546-15760">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_4.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_5.png">
</figure>
</div>
<ul>
  <li md-pos="15762-15794">Efficient Grid Size Reduction</li>
</ul>
<p md-pos="15795-15875"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_6.png" alt="" md-pos="15795-15874" /></p>
<h4 id="experiment-details-2" md-pos="15881-15899"><a href="#experiment-details-2" name="experiment-details-2">Experiment details</a></h4>
<ul>
  <li md-pos="15900-15944">batch size 32 for 100 epochs,momentum=0.9</li>
  <li md-pos="15944-15989">RMSProp with decay of 0.9 and $\in$= 1.0.</li>
  <li md-pos="15989-16075">learning rate of 0.045, decayed every two epoch using an exponential rate of 0.94.</li>
</ul>
<h4 id="experiment-result-1" md-pos="16081-16098"><a href="#experiment-result-1" name="experiment-result-1">Experiment result</a></h4>
<p md-pos="16100-16180"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv3_7.png" alt="" md-pos="16100-16179" /></p>
<h4 id="some-questions" md-pos="16186-16200"><a href="#some-questions" name="some-questions">Some questions</a></h4>
<ul>
  <li md-pos="16201-16310">What is the meaning of 'Higher dimensional representations are easier to process locally within a network'</li>
  <li md-pos="16310-16486">why high quality results can be reached with receptive field resolution as low as 79×79.
    This might prove to be helpful in systems for detecting relatively small objects.</li>
</ul>
<h1 id="deep-residual-learning-for-image-recognition2015" md-pos="16491-16541"><a href="#deep-residual-learning-for-image-recognition2015" name="deep-residual-learning-for-image-recognition2015">Deep Residual Learning for Image Recognition(2015)</a></h1>
<div md-pos="16543-16752">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet1.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_formula.png">
</figure>
</div>
<p md-pos="16754-16828"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet2.png" alt="" md-pos="16754-16827" /></p>
<h4 id="design-rules" md-pos="16834-16847"><a href="#design-rules" name="design-rules">Design rules:</a></h4>
<ul>
  <li md-pos="16848-16931">For the same output feature map size, the layers have the same number of filters</li>
  <li md-pos="16931-17050">If the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.</li>
  <li md-pos="17050-17065">conv-bn-relu</li>
</ul>
<h4 id="some-shortness-of-deeper-networks" md-pos="17071-17105"><a href="#some-shortness-of-deeper-networks" name="some-shortness-of-deeper-networks">Some shortness of deeper networks：</a></h4>
<ul>
  <li md-pos="17107-17139">深层的网络梯度容易消失(can solved by BN)</li>
  <li md-pos="17139-17227">With the network depth increasing, accuracy gets saturated and then degrades rapidly.</li>
</ul>
<h4 id="resnet-improves" md-pos="17233-17249"><a href="#resnet-improves" name="resnet-improves">Resnet improves:</a></h4>
<ul>
  <li md-pos="17250-17339">Ease the training of networks that are substantially deeper than those used previously</li>
  <li md-pos="17339-17516">更快收敛 Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases</li>
  <li md-pos="17516-17670">误差更小 Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks</li>
</ul>
<h4 id="experiment-details-3" md-pos="17676-17694"><a href="#experiment-details-3" name="experiment-details-3">Experiment details</a></h4>
<ul>
  <li md-pos="17695-17780">Data augmentation: randomly sampled, scale augmentation, per-pixel mean subtracted</li>
  <li md-pos="17780-17881">SGD(lr=0.1, divided by 10 when the error plateaus, weight decay of 0.0001 and a momentum of 0.9. )</li>
  <li md-pos="17881-17906">mini-batch size of 256</li>
  <li md-pos="17906-17924">epoch= 60×10^4</li>
  <li md-pos="17924-17943">without dropout</li>
  <li md-pos="17943-18015">test: standard 10-crop testing, average the scores at multiple scales</li>
</ul>
<h1 id="identity-mappings-in-deep-residual-networks2016" md-pos="18018-18067"><a href="#identity-mappings-in-deep-residual-networks2016" name="identity-mappings-in-deep-residual-networks2016">Identity Mappings in Deep Residual Networks(2016)</a></h1>
<p md-pos="18069-18086">2 improvements:</p>
<ul>
  <li md-pos="18086-18119">Identity shortcut connections</li>
  <li md-pos="18119-18156">Identity after-addition activation</li>
</ul>
<p md-pos="18159-18240"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_compare.png" alt="" md-pos="18159-18239" /></p>
<p md-pos="18243-18263">origin vs proposed:</p>
<div md-pos="18265-18495">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/res_origin_formula.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_fproposed_formula.png">
</figure>
</div>
<p md-pos="18497-18505">result:</p>
<p md-pos="18506-18586"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_result.png" alt="" md-pos="18506-18585" /></p>
<h4 id="why-it-works-3" md-pos="18592-18604"><a href="#why-it-works-3" name="why-it-works-3">Why it works</a></h4>
<ul>
  <li md-pos="18605-18701">The forward and backward signals can be directly propagated from one block to any other block</li>
</ul>
<p md-pos="18702-18784"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_backward.png" alt="" md-pos="18702-18783" /></p>
<ul>
  <li md-pos="18785-18872" class="p">
    <p md-pos="18787-18872" class="p">The gradient of a layer does not vanish even when the weights are arbitrarily small.</p>
  </li>
  <li md-pos="18872-19236" class="p">
    <p md-pos="18874-18896" class="p">Ease of optimization：</p>
    <p md-pos="18902-19002">Using the original design, the training error is reduced very slowly at the beginning of training.</p>
    <p md-pos="19009-19136">For f = ReLU, the signal is impacted if it is negative, and when there are many Residual Units, this effect becomes prominent.</p>
    <p md-pos="19143-19236">When f is an identity mapping, the signal can be propagated directly between any two units.</p>
  </li>
  <li md-pos="19236-19595" class="p">
    <p md-pos="19238-19259" class="p">Reducing overfitting</p>
    <p md-pos="19264-19376">The pre-activation version reaches slightly higher training loss at convergence, but produces lower test error.</p>
    <p md-pos="19381-19438">This is presumably caused by BN’s regularization effect.</p>
    <p md-pos="19443-19595">In the original Residual Unit, although the BN normalizes the signal, this is soon added to the shortcut and thus the merged signal is not normalized.</p>
  </li>
</ul>
<h4 id="some-ablation-experiments-on-shortcut-connections" md-pos="19600-19650"><a href="#some-ablation-experiments-on-shortcut-connections" name="some-ablation-experiments-on-shortcut-connections">Some ablation experiments on shortcut connections:</a></h4>
<p md-pos="19652-19748">conclusion:the Shortcut connections are the most direct paths for the information to propagate.</p>
<p md-pos="19749-19914">Multiplicative manipulations (scaling, gating, 1×1 convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems.</p>
<ul>
  <li md-pos="19914-20027" class="p">
    <p md-pos="19916-19933" class="p">Constant scaling</p>
    <p md-pos="19936-20027">Suggesting that the optimization has difficulties when the shortcut signal is scaled down.</p>
  </li>
  <li md-pos="20027-20110" class="p">
    <p md-pos="20029-20046" class="p">Exclusive gating</p>
    <p md-pos="20052-20110">The impact of the exclusive gating mechanism is two-fold.</p>
  </li>
  <li md-pos="20110-20262" class="p">
    <p md-pos="20112-20139" class="p">1×1 convolutional shortcut</p>
    <p md-pos="20144-20262">When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation.</p>
  </li>
  <li md-pos="20262-20338" class="p">
    <p md-pos="20264-20283" class="p">Dropout shortcut</p>
    <p md-pos="20288-20338">The network fails to converge to a good solution.</p>
  </li>
</ul>
<h4 id="some-ablation-experiments-on-activation-functions" md-pos="20343-20392"><a href="#some-ablation-experiments-on-activation-functions" name="some-ablation-experiments-on-activation-functions">Some ablation experiments on activation functions</a></h4>
<p md-pos="20394-20478"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_experiment.png" alt="" md-pos="20394-20477" /></p>
<ul>
  <li md-pos="20479-20759" class="p">
    <p md-pos="20481-20499" class="p">BN after addition</p>
    <p md-pos="20504-20561">The results become considerably worse than the baseline.</p>
    <p md-pos="20566-20759">The BN layer alters the signal that passes through the shortcut and impedes information propagation,
    as reflected by the difficulties on reducing training loss at the beginning of training.</p>
  </li>
  <li md-pos="20759-20933" class="p">
    <p md-pos="20761-20782" class="p">ReLU before addition</p>
    <p md-pos="20786-20933">As a result, the forward propagated signal is monotonically increasing.
    This may impact the representational ability, and the result is worse.</p>
  </li>
  <li md-pos="20933-21267" class="p">
    <p md-pos="20935-20969" class="p">Post-activation or pre-activation</p>
    <p md-pos="20973-21060">The ReLU-only pre-activation performs very similar to the baseline on ResNet-110/164.</p>
    <p md-pos="21067-21165">This ReLU layer is not used in conjunction with a BN layer, and may not enjoy the benefits of BN.</p>
    <p md-pos="21172-21267">When BN and ReLU are both used as pre-activation, the results are improved by healthy margins.</p>
  </li>
</ul>
<h1 id="inception-v4inception-resnet-and-the-impact-of-residual-connections-on-learning2016" md-pos="21273-21359"><a href="#inception-v4inception-resnet-and-the-impact-of-residual-connections-on-learning2016" name="inception-v4inception-resnet-and-the-impact-of-residual-connections-on-learning2016">Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning(2016)</a></h1>
<p md-pos="21360-21422">combine the Inception architecture with residual connections.</p>
<div md-pos="21423-21638">
   <figure class="half">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/Inceptionv4.png">
    <img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/Inception-ResNet.png">
</figure>
</div>
<p md-pos="21640-21772">Give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly.</p>
<p md-pos="21773-21904">Studied how the introduction of residual connections leads to dramatically improved training speed for the Inception architecture.</p>
<h4 id="some-doubts-on-hes-point" md-pos="21910-21935"><a href="#some-doubts-on-hes-point" name="some-doubts-on-hes-point">Some doubts on He's point</a></h4>
<ul>
  <li md-pos="21936-22051">Residual connections are inherently necessary for training very deep convolutional models for image recognition.</li>
  <li md-pos="22051-22176">Demonstrate that it is not very difficult to train competitive very deep networks without utilizing residual connections.</li>
</ul>
<h4 id="the-evolution-of-inception" md-pos="22182-22208"><a href="#the-evolution-of-inception" name="the-evolution-of-inception">The evolution of Inception</a></h4>
<ul>
  <li md-pos="22209-22237" class="p">
    <p md-pos="22211-22237" class="p">GoogLeNet or Inception-v1</p>
  </li>
  <li md-pos="22237-22259" class="p">
    <p md-pos="22239-22259" class="p">Inception-v2 add BN</p>
  </li>
  <li md-pos="22259-22299" class="p">
    <p md-pos="22261-22299" class="p">Inception-v3 add factorization ideas</p>
  </li>
  <li md-pos="22299-22590" class="p">
    <p md-pos="22301-22363" class="p">Inception-v4 use cheaper Inception blocks combine with resnet</p>
    <p md-pos="22366-22590">Each Inception block is followed by filter-expansion layer (1 × 1 convolution without activation)
    which is used for scaling up the dimensionality
    of the filter bank before the addition to match the depth of the input.</p>
  </li>
</ul>
<h4 id="improvement-of-hes-resnetscaling-of-the-residuals" md-pos="22598-22649"><a href="#improvement-of-hes-resnetscaling-of-the-residuals" name="improvement-of-hes-resnetscaling-of-the-residuals">Improvement of He's Resnet:Scaling of the Residuals</a></h4>
<p md-pos="22652-22731"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/resnet_scale.png" alt="" md-pos="22652-22730" /></p>
<ul>
  <li md-pos="22732-22760">To stabilize the training</li>
</ul>
<h4 id="experiment-details-4" md-pos="22766-22784"><a href="#experiment-details-4" name="experiment-details-4">Experiment details</a></h4>
<ul>
  <li md-pos="22785-22829">RMSProp(momentum=0.9,decay=0.9,$\in$=1.0)</li>
  <li md-pos="22829-22900">lr=0.045,decayed every two epochs using an exponential rate of 0.94</li>
</ul>
<p md-pos="22901-22986"><img src="https://github.com/SherlockHolmes221/CNNs/raw/master/img/inceptionv4_result.png" alt="" md-pos="22901-22985" /></p>
<ul>
  <li md-pos="22987-23112">The residual version was training much faster and reached slightly better ﬁnal accuracy than the traditional Inception-v4.</li>
  <li md-pos="23112-23223">Although the residual version converges faster, the final accuracy seems to mainly depend on the model size.</li>
</ul>
<h4 id="some-questions-about-the-point-mentioned-in-the-paper" md-pos="23229-23283"><a href="#some-questions-about-the-point-mentioned-in-the-paper" name="some-questions-about-the-point-mentioned-in-the-paper">Some Questions about the point mentioned in the paper:</a></h4>
<ul>
  <li md-pos="23284-23510">Each Inception block is followed by filter-expansion layer (1 × 1 convolution without activation)
    which is used for scaling up the dimensionality
    of the filter bank before the addition to match the depth of the input.</li>
  <li md-pos="23510-23560">The details about the Scaling of the Residuals.</li>
</ul>
</div>
</body>
</html>
